<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriately as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="RD-VIO: Robust visual-inertial odometry for mobile augmented reality in dynamic environments">
  <meta property="og:title" content="RD-VIO: Robust Visual-Inertial Odometry"/>
  <meta property="og:description" content="RD-VIO: Robust visual-inertial odometry for mobile augmented reality in dynamic environments"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="RD-VIO: Robust Visual-Inertial Odometry">
  <meta name="twitter:description" content="RD-VIO: Robust visual-inertial odometry for mobile augmented reality in dynamic environments">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="computer vision, visual-inertial odometry, augmented reality, SLAM, RD-VIO, mobile AR">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>RD-VIO</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/js/all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
            <h1 class="title is-3 publication-title">
              RD-VIO:
              <span style="color: red; font-weight: bold;">R</span>obust
              <span style="color: red; font-weight: bold;">V</span>isual-
              <span style="color: red; font-weight: bold;">I</span>nertial 
              <span style="color: red; font-weight: bold;">O</span>dometry for Mobile Augmented Reality in 
              <span style="color: red; font-weight: bold;">D</span>ynamic Environments
            </h1>
            
            <div class="is-size-6 has-text-centered" style="margin-top: 1rem; margin-bottom: 1.5rem;">
              <strong>Note:</strong> RD-VIO is an important module of
              <a href="https://github.com/openxrlab" target="_blank">OpenXRLab</a>.
              For more details, please refer to <a href="https://github.com/openxrlab/xrslam" target="_blank">xrslam</a>.
            </div>
            
            <div class="is-size-6 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/itsuhane" target="_blank">Jinyu Li</a><sup>1*</sup>,
              </span>
              <span class="author-block">
                <a href="https://github.com/panxkun" target="_blank">Xiaokun Pan</a><sup>1*</sup>,
              </span>
              <span class="author-block">
                <a href="https://github.com/huanggan52" target="_blank">Gan Huang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                Ziyang Zhang<sup>1</sup>,
              </span>
              <span class="author-block">
                Nan Wang<sup>2</sup>,
              </span>
              <span class="author-block">
                Hujun Bao<sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="http://www.cad.zju.edu.cn/home/gfzhang/" target="_blank">Guofeng Zhang</a><sup>1†</sup>
              </span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block">
                <sup>1</sup>State Key Lab of CAD&amp;CG, Zhejiang University<p>
                <sup>2</sup>SenseTime Research<br>
              <span style="font-weight:bold; color:#53615f; font-size:1.0em;">TVCG 2024</span>
              </span>
              </span>
              <br>
              <span class="author-block">
                * Equal Contribution &nbsp;&nbsp; † Corresponding author
              </span>
            </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv PDF link -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2310.15072.pdf" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-alt"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              
              <!-- ArXiv abstract Link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2310.15072" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/openxrlab/xrslam" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-code"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div style="text-align: center;">
        <img src="assets/teaser.png" alt="RD-VIO Banner" style="max-width:100%; height:auto;">
      </div>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            It is typically challenging for visual or visual-inertial odometry systems to handle the problems of dynamic scenes and pure rotation. In this work, we design a novel visual-inertial odometry (VIO) system called RD-VIO to handle both of these two problems. Firstly, we propose an IMU-PARSAC algorithm which can robustly detect and match keypoints in a two-stage process. In the first state, landmarks are matched with new keypoints using visual and IMU measurements. We collect statistical information from the matching and then guide the intra-keypoint matching in the second stage. Secondly, to handle the problem of pure rotation, we detect the motion type and adapt the deferred-triangulation technique during the data-association process. We make the pure-rotational frames into the special subframes. When solving the visual-inertial bundle adjustment, they provide additional constraints to the pure-rotational motion. We evaluate the proposed VIO system on public datasets and online comparison. Experiments show the proposed RD-VIO has obvious advantages over other methods in dynamic environments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-4">Video</h2>
      
      <div class="has-text-centered" style="margin-bottom: 2rem;">
        <video controls style="max-width:100%; height:auto;">
          <source src="assets/video/rdvio-demo.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>

      </div>
    </div>
  </section>


<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-4">BibTeX</h2>
    <pre>
@article{li2024rd,
  title={RD-VIO: Robust visual-inertial odometry for mobile augmented reality in dynamic environments},
  author={Li, Jinyu and Pan, Xiaokun and Huang, Gan and Zhang, Ziyang and Wang, Nan and Bao, Hujun and Zhang, Guofeng},
  journal={IEEE transactions on visualization and computer graphics},
  volume={30},
  number={10},
  pages={6941--6955},
  year={2024},
  publisher={IEEE}
}
</pre>
  </div>
</section>
<!--End BibTex citation -->

<footer class="footer">
  <div class="footer-content">
    <div class="content">
      <p>
        © 2024 Me. Adapted from <a href="https://github.com/nerfies/nerfies.github.io" target="_blank">Nerfies</a> (CC BY-SA 4.0). Modified with GPT-4.0.
      </p>
    </div>
  </div>
</footer>


<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>
</html>
